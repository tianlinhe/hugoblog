---
title: "Three methods to determine confidence interval (CI) for ROC curve in Python"
subtitle: "Python codes of boostrap and popular non-bootstrap methods"
date: 2019-10-06T09:59:03+02:00
draft: false
description: calculate CI for AUC in ROC curve with bootstrapping, Delong or McNeil methods
tags: ["python","scikit-learn","confidence interval","bootstrapping","ROC","AUC"] 

---

## Scenario:
With the set of scores and binary class labels, you succesfully plotted the ROC (Receiver Operative Characteristic ) curve and computed its AUC (area und curve) with `scikit-learn` from python:  


```python
import numpy as np
#y_pred: numpy array of scores predicted by your model
y_pred = np.array([0.8556125820831081, 0.1638014063083847, ...,
                   0.08512761636008265])

#y_true: numpy array of original class labels of your dataset
y_true = np.array([1, 1,..., 0]) 

from sklearn.metrics import roc_auc_score
print("Original ROC area: {:0.3f}".format(roc_auc_score(y_true, y_pred)))
```
Now you want to determint the 95% confidence interval (CI) of the AUC, which you would later need for other purposes (validation, publication...). How can the mission be accomplished in **python**?

The blog guides you through calculating CI for AUC with methods in python codes:

- [x] Bootstrap
- [ ] McNeil mthod
- [ ] Delong method

## 1. Bootstrapping
Before proceeding to the actual code work, let's briefly discuss some principles of bootstrapping. First, "bootstrapping" here means "random resampling without replacement". Here is an example of bootstrapping the ages the family members, which is a 6\*2 table:

|Members|Age|
|------------ | -------------|
|Grandpa| 75|
|Grandma| 72|
|Father | 48|
|Mother | 49|
|Agnes| 12|
|Agnes' cat|4|


After a single bootstrap we will obtain another  6\*2 table:

|Members| 	Age|
|------------ | -------------|
|Grandpa| 	75|
|Father | 	48|
|Agnes' cat|	4|
|Agnes| 	12|
|Agnes' cat|	4|
|Grandma| 	72|
Do you realize that Agnes' cat exists twice? It is because each individual draw (of a family member) is **independent**  from the previous draw (what we had in the generating table). Maths tells us that there can be as many as 6^ 6 ^=46656 possible tables generated. So, if we repeat bootstrapping, we will probably adifferent table each time, which infers the second property of boostrapping: **randomness**.

Now back to our score-label business. We are going to
1. Repeat boostrapping for the dataset for several times
2. For each dataset generated by bootstrapping, calculate the AUC for its ROC curve
3. Look at the distribution of all the AUC values we obtained
4. 95% CI corresponds to the 5% and 95% percentile of the AUC distribution

I wrapped steps 1 and 2 in python inside `def bootstrap`:
```python
import numpy as np
from scipy.stats import sem
from sklearn.metrics import roc_auc_score

def ci_bootstrap(score, label):
###n=2000 is the default of R package pROC, for estimation of 2 sig. fig
    n_bootstraps = 2000 #number of bootstraps
    rng_seed = 42  # for reproducibility of bootstrapping
    bootstrapped_scores = []

    rng = np.random.RandomState(rng_seed)
    for i in range(n_bootstraps):
    # bootstrap by sampling with replacement on the prediction indices
        indices = rng.randint(0, len(score)-1,len(score))
        if len(np.unique(label[indices])) < 2:
        # We need at least one positive and one negative sample to be defined
            continue
        
        auc = roc_auc_score(label[indices], score[indices])
        bootstrapped_scores.append(auc)
  
    return bootstrapped_scores
```
Here you go! The bootstrapped scores are ready once you call `ci_bootstrap`. To continue with the confidence interval:
```python
###95% quantile of bootstrapped scores
ci095=np.quantile(ci_bootstrap(y_pred,y_true),0.95)
###5% quantile of bootstrapped scores
ci005=np.quantile(ci_bootstrap(y_pred,y_true),0.05)

print ("AUC is {:0.3f}".format(roc_auc_score(y_true, y_pred)),"with 95% CI from {:0.3f} to {:0.3f}".format(ci005,ci095))
```
With 2000 bootstraps indicated in this example, the output is:
```AUC is 0.833 with 95% CI from 0.736 to 0.904```
### Summary of bootstrapping
* Adv: **no assumptions** in sample distribution neede
* Disadvantage: can be **computationally intensive** when the dataset is large

Therefore, the next two methods to be introduced require far less computation and are widely used in statistical tools such as R and MedCal.

## 2. McNeil method





â€‹    


